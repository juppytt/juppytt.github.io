[{"content":"Hello, my name is Juhee Kim, and I am a PhD student at CompSec Lab, Department of Electrical and Computer Engineering, Seoul National University. I am advised by Byoungyoung Lee.\nMy research interests span a broad spectrum of software security issues and finding practical solutions. I\u0026rsquo;m interested in Linux kernel, Web browsers, and ML systems.\nUpdates I am going to present our ARM MTE side-channel attack in Black Hat USA 2024! We arxived our paper, TikTag. Publications Juhee Kim, Jinbum Park, Sihyeon Roh, Jaeyoung Chung, Youngjoo Lee, Taesoo Kim, and Byoungyoung Lee TikTag: Breaking ARM\u0026rsquo;s Memory Tagging Extension with Speculative Execution arXiv 2024 Adil Ahmad, Juhee Kim, Jaebaek Seo, Insik Shin, Pedro Fonseca, and Byoungyoung Lee CHANCEL: Efficient Multi-client Isolation Under Adversarial Programs Network and Distributed System Security Symposium (NDSS) 2021 Education Ph.D. in Department of Electrical and Computer Engineering, Seoul National University 2020.03 - Present B.S. in Department of Electrical and Computer Engineering, Seoul National University 2015.03 - 2020.02 Things I love Swimming and Yoga Reading books, going to the library Writing journal, research paper Listening to music (I\u0026rsquo;m recently deeply listening to Japanese bands) ","href":"/","title":"Home"},{"content":"","href":"/archive/","title":"Archive"},{"content":"It\u0026rsquo;s the end of 2023, and as I look back on the year, I can only say that it was a year full of research. I remember the days spent in front of the computer, reading papers, and conducting experiments. Of course, there were other events this year, including the creation of this blog (yay!). So, I decided it would be nice to write a review of 2023, to reflect on this year and to prepare for the next year.\nSurprises of 2023 Things that happened in 2023 that I didn\u0026rsquo;t expect.\nA new research topic - LLM security The most significant technical surprise of 2023 might be the development of ChatGPT. I found ChatGPT and its use of plugins, along with its interaction with Non-ML programs, to be very interesting. I decided to investigate more on this topic and realized it was a new research area that I could work on. Delving into this topic, I somehow ended up understanding more about old-school security models and information flow control. This even led me to gain deeper understanding on the security papers I read previously. Currently working on other projects, but I am eager to continue on this topic as soon as I finish them!\nNew research collaboration. This year, I was fortunate to have a new collaboration with excellent researchers to enhance the previously rejected paper. I am very proud of the work we did together and hope that it will be accepted. It was a great experience. Lesson learned: Reaching out brings new opportunities.\nAnother new research topic - ARM MTE side-channel From the previous collaboration, I also got a chance to work on a new research topic: ARM MTE side-channel (officially disclosed this month!) I am very excited on this project, and I hope that we can complete it with a good result.\nTriumphs of 2023 Things that I am proud of in 2023.\nMorning routine I have been trying to wake up early for a long time. Starting a day early is a common advice for productivity, and for me, it was about avoiding the morning traffic jam in Seoul. The only working solution was the morning exercise routine. I enjoyed morning swimming from March to November and switched to yoga from December. This habit allows me to arrive at the lab earlier than others and have my own time focusing on my work. I\u0026rsquo;m very pleased with this routine and hope to continue it next year. I believe no one who knows me would believe that I am now a morning person. Still, I ocassionally oversleep, so my goal for the next year is to establish a more consistent morning routine.\nNew blog I launched this blog this summer. Although I haven\u0026rsquo;t written much yet, it is good to have a place to write. I admit that I am still shy about sharing my ideas publicly, and I haven\u0026rsquo;t managed to write regularly. However, I hope to write more in the next year.\nStudying English I re-joined the English speaking group I participated years ago and felt my English has quite improved. I consistently took italki lessons as well. One day, I accidentally joined a group class for C1 level students. The students really challenged me, letting me realize that I still have a long way to go.\nUnfulfilled goals of 2023 Things that I wanted to do in 2023 but failed to do so.\nGetting paper accepted The paper I submitted this year faced rejection, despite I believed I had done everything I could. It was a puzzling result. My advisor reassured me that it was a good paper and if I keep working on it, it would be accepted one day.. Anyway, I quickly re-submitted to another venue. After that, I realized my previous rejection was due to failing to satisfy one reviewer\u0026rsquo;s seemingly trivial demands. It is painful to go through another round of review process, and it was a worthwhile lesson learned. With several other interesting projects in progress, I am optimistic that I\u0026rsquo;ll have more opportunities to achieve this goal in the coming year.\nOthers Reading books: Started multiple books but failed to finish any of them..;-) Consistent wake-up time: My sleep schedule is still inconsistent. Establishing a consistent sleep schedule is next year\u0026rsquo;s goal. Being nice to others: This one wasn\u0026rsquo;t actually planned, but I realized this year it would be beneficial to everyone if I am a bit nicer. ","href":"/posts/2023-review/","title":"A review of 2023: Surprises, Triumphs, and Failures"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/categories/graduate-life/","title":"Graduate Life"},{"content":"","href":"/posts/","title":"Posts"},{"content":"","href":"/tags/productivity/","title":"productivity"},{"content":"Time flies in graduate school! Having spent 3.5 years as a PhD student, one thing I\u0026rsquo;ve learned is that I should be super-efficient in managing my time and energy.\nI\u0026rsquo;m not someone you\u0026rsquo;d label as \u0026ldquo;born productive\u0026rdquo;. I\u0026rsquo;m a typical \u0026ldquo;P\u0026rdquo;-type person according to Myers-Briggs Type Indicator (MBTI)\u0026ndash;indicative of a spontaneous and flexible nature. As many P-type people do, I\u0026rsquo;m prone to distraction and have difficulty in sticking to routines. Daily to-do lists or planners are ways to self-torture by confining free will into a fixed schedule. I also sleep a lot and have a hard time waking up early.\nIn my undergraduate years, I\u0026rsquo;d often find myself pulling all-nighters to finish assignment projects and study for the exams. Yet, despite this approach, I managed to get average grades and was lucky enough to join an awesome research group for my PhD.\nResearch, however, is a different game. It demands consistent and sustained effort\u0026ndash;ideation, problem-solving, experimentation, and writing a paper. Especially when you are targeting top-tier conferences, they would require creativity and novelty in your work, which cannot be achieved over a single night. Speed is essential too; the review process takes months and there\u0026rsquo;s always a chance that you are not the only one who thought of the same idea.\nWhat\u0026rsquo;s more, graduate life is not only about research. There are classes, seminars, TAs, meetings, conferences, paperwork, and so on. Of course, you also have to study English and maintain your health and social life.\nIn the first two years of PhD, I was either busy working overnight to meet the deadlines or wasting my time on random things that intrigued my interest yet were not so important. I was constantly stressed out and felt that I was not productive enough. I was also nervous about my academic performance and my future career since I was not sure if I could finish my PhD with good publications.\nIn 2022, my third year, I decided to bring some change to my life. I experimented with various productivity strategies from experts to find what worked for me. Today, while I\u0026rsquo;m still in exploration, I discovered several ways that boost my productivity. While there\u0026rsquo;s no assurance that they\u0026rsquo;ll bring me a list of high-quality papers, at least I feel I have done my best of what I can do. No more regrets.\nThrough this series, I\u0026rsquo;ll share some productivity insights that have proven to work for me. I hope this series helps some readers who are struggling with their productivity concerns.\nSide Note: Took the MBTI test again and I\u0026rsquo;m now 57% J-type. I guess I\u0026rsquo;m going in the right direction? :)\n","href":"/posts/series-productivity/","title":"Series: Graduate School Productivity - Introduction"},{"content":"","href":"/tags/","title":"Tags"},{"content":" Introduction In recent years, emerging technology in Machine Learning and Large Language Model (LLM) had led to the development of ML/LLM-powered applications. One of the popular service for ML/LLM application is Hugging Face. In this post, I would like to describe the potential security risks in current Hugging Face, especially in Spaces.\nHugging Face Hugging Face is a AI community that provides a lot of machine learning models, datasets, and applications. Users can contribute their own models, datasets, and demo applications (i.e., Spaces) to the community. This community has been growing rapidly since the advent of the transformer model.\nSpecifically, Spaces offers a easy-to-user environment to manage and host demo applications for models. Users can upload and deploy Spaces, which is then served as MLaaS (Machine Learning as a Service) from Hugging Face\u0026rsquo;s server. Spaces support various interface SDKs (Gradio, Streamlit) and execution environment options with various GPU accelerators.\nSecurity Risks on Spaces Despite its usability, Spaces incorporates several vulnerable designs, which may potentially lead to security issues.\nVuln. Design 1: Unrestricted App Logics First, Spaces allow users to upload and distribute any kind of apps. This unrestricted deployment introduces potential unsafe data-flows between the user and the app (or the execution environment). Specifically, arbitrary code execution and information leakage.\nIssue 1: Arbitrary Code Execution By its nature, Spaces allow arbitrary code execution from app developers to Hugging Face server. Developers can run any app on Hugging Face\u0026rsquo;s server just by uploading the app to Spaces. Moreover, if the app contains data-flows from user input to executable data (e.g., user-provided functions or shell commands), it would allow arbitrary code executions from app users to the server. If the app is publicly available, the arbitrary code execution functionality would be exposed to unspecified and anonymous users.\nWith proper containerization, the effect of arbitrary code execution is limited to the sandboxed environment. However, if a container escape vulnerability might allow the attacker to attempt container escape attack, which would further enable attacks on the host server.\nArbitrary code execution also allows attackers to change app\u0026rsquo;s behavior or hijack the goal. One example is bypassing any security measurement (e.g., safeguard prompts, throughput). With arbitrary execution, the attackers can delete or replace the safeguard prompts passed to the model, or they can nullify the restrictions on users such as throughput.\nAnother example is replacing the model used by the app with different model (e.g., backdoor-injected model, model for different tasks). Considering high-end GPU acceleration environment charge extra costs for the app developer, such attempts might also yield financial loss.\nReal-World Example shell is a simple app that takes user input and passes it to a langchain tool called ShellTool, which runs the input as a shell command. By sending commands, users can explore or manipulate the execution environment. For instance, sending ls as the command input would return the actual file in the containerized environment.\narbitrary shell command execution in Hugging Face Spaces\nIssue 2: Information Leakage between App and User Many apps in Spaces operate with numerous user-provided sensitive data. Apps require user credentials (e.g., API keys, ID, password) as a delegate for API uses. Apps require personal information (e.g., name, email, location) to provide customized services. When processing such data, apps should be safely designed to protect personal information. Recent privacy regulations (e.g., GDPR, CCPA) consider any user-provided input passed to web apps as sensitive data and enforces that appsq should not store nor track such data without user content.\nLacking restriction on app internal logics may allow dataflow from the users to untrusted channels. When user data is passed to an app, the app should carefully process the data for the intended operations only. However, as Spaces allow any apps with any logic to be deployed, we cannot assure whether the apps are properly handling user data.\nReal-World Example nlp-goemotions-senti-pred is a demo app for a sentiment prediction model, which allows user to input text and receive sentiment predictions from the model. For instance, if a user inputs \u0026ldquo;I trust you\u0026rdquo;, the app will return \u0026ldquo;agreement\u0026rdquo; as the predicted sentiment.\nnlp-goemotions-senti-pred predicts sentiment from input sentence\nHowever, there is a hidden functionality in this app that logs every user input and updates it to a public Huggingface dataset maintained by the app-developer. Consequently, every text provided by users is leaked and publicly exposed. The app does not inform users about this background activity, leaving them unaware of their data being logged into a dataset.\nUser input is silently logged into another dataset\nVuln. Design 2: Missing Isolation between Requests Another vulnerable design in Spaces is the lack of sufficient isolation between user requests. Currently, Spaces uses a single container when running an app. Therefore, every request made to the app, regardless of the sender, is executed within the same container environment. This can lead to unsafe data-flows between users, such as information leakage.\nIssue 3: Information Leakage between Users As Spaces provide a single container per app, it is crucial to ensure that user data is not shared between requests. If an app has a data-flow that stores user input to global states and then loads those global states into user-visible outputs, the app becomes vulnerable to user information leakage.\nTo illustrate this vulnerability, let\u0026rsquo;s take an example using shell. If a user executes the command echo \u0026quot;12345\u0026quot; \u0026gt; file \u0026amp;\u0026amp; ls, the app will create a file named file with 12345 and list the existing files in the current directory.\nSpaces allow apps to store user data as a global state (e.g., file)\nThen, another user can access the file by sending commands. An arbitrary user can read file by passing cat file to the app, thus leaking the data provided by the previous user.\nSharing a container between requests allows users to leak other users\u0026rsquo; data\nReal-World Example Chatbot-Blenderbot is a chat bot app that, upon receiving a user request, runs a model and saves every user input and model response to a file. The entire file, including inputs from other users, is then displayed to the user. However, this app does not notify users that their input will be logged and exposed to other users. Users only become aware that their input is being shared with others after they have already sent their data to the app.\nChatbot-Blenderbot exposes every input from different users without user consent\nConclusion I reported these issues to Hugging Face security team through email and Hugging Face forum. They didn\u0026rsquo;t replied the email but did show interest in addressing the issues through the forum. Unfortunately, they have not yet provided any working solutions or mitigation measures.\nAs we are in the initial stage of ML/LLM app development, there are several security vulnerabilities similar to those commonly studied in non-ML programs. One good news is that current ML/LLM apps are mostly for performing simple tasks like text/image transformation or chatbot functionalities, which are less prone to severe security vulnerabilities. Nonetheless, as this field continues to develop rapidly, applications will become complex, and a wide range of security issues are likely to emerge.\n","href":"/posts/hugging-evil-face/","title":"Hugging (Evil) Face: Security issues in Hugging Face"},{"content":"","href":"/tags/hugging-face/","title":"Hugging Face"},{"content":"","href":"/categories/research/","title":"Research"},{"content":"","href":"/tags/kernel/","title":"kernel"},{"content":"","href":"/tags/llvm/","title":"LLVM"},{"content":"","href":"/tags/qemu/","title":"QEMU"},{"content":"Intro In the first year of my Ph.D, I ambitiously began a research on protecting the Linux kernel against data-only attacks. My research especially focused on protecting security-critical objects and pointers leveraging the ARM\u0026rsquo;s upcoming hardware security features (PAC and MTE).\nBuilding such a kernel hardening system required me to tackle several key issues:\nCross-compiling the kernel (X86 -\u0026gt; ARM64) Cross-module kernel analysis Linux kernel instrumentation Connecting the cross-module analysis with kernel build system. Setting up ARM64 environments for both emulation and a real device (Raspberry Pi 4B). As someone who was not well familar with kernel and compiler, each of these issues presented quite a challenge. So I thought it would be nice to write about each of the issues to help others facing similar issues.\n0. Environment My environment was as follows:\nHost Environment: Linux 5.15 Ubuntu 20.04.3 Clang/LLVM: 11.0.0 and 14.0.0 GNU Toolchain: aarch64-none-linux-gnu Qemu 5.1.0 Raspberry Pi 4B (4GB) 1. Building Kernel As the target system is AArch64 architecture and the host system is x86_64, the kernel needs to be cross-compiled from x86_64 to AArch64. At first, I expected Clang/LLVM would be able to do this job with -march option, as it is natively a cross-compiler. However, there was some problems related to the known cross compilation issues of Clang/LLVM. As described in ClangBuiltLinux and a blog, using ARM\u0026rsquo;s GNU toolchain (aarch64-none-linux-gnu) solves the problem.\nThe final script I used to cross-compile the kernel:\n#!/bin/bash -ve PROJ_DIR=${PWD}/.. LLVM_BUILD=${PROJ_DIR}/build/bin BINUTIL=~/util/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin export PATH=${BINUTIL}:$PATH:${LLVM_BUILD} export LLVM_COMPILER=clang export KERNEL=kernel8 export KCFLAGS=\u0026#34;-march=armv8.5-a \u0026#34; pushd ../linux-5.5 BINUTILS_TARGET_PREFIX=aarch64-none-linux-gnu make \\ ARCH=arm64 CROSS_COMPILE=aarch64-none-linux-gnu- \\ HOSTCC=clang CC=clang -j12 popd 2. Cross-module kernel analysis To identify every instructions accessing the protection target, a cross-module data-flow analysis is performed. Here, module indicates a single C file. One approach is to use wllvm to generate a single LLVM bitcode file of the entire kernel (i.e., vmlinux.bc). Here, some kernel modules containing duplicate symbol names may occur linking errors. Such files can be manually excluded from vmlinux.bc as explained in this blog, while such modules will be excluded from the analysis.\nOnce vmlinux.bc is generated, a custom LLVM analysis pass can be applied to it, enabling the cross-module analysis. The analysis pass is implemented as a LLVM pass plugin, which is loaded by opt command. Given a list of protection target (e.g., struct types, named global variables), the analysis pass identifies the memory access instructions (i.e., load/store, copy, alloc/free) that access the protection target.\n3. Linux Kernel Instrumentation The instrumentation is applied during kernel build on the instructions identified from the analysis. The LLVM tutorial on writing a LLVM pass introduces writing a LLVM pass as a plugin tool for opt command. However, the kernel build system uses CC (clang) and LD (lld) to build the kernel, instead of opt.\nOne approach is -Xclang option to pass the instrumentation pass as a Clang pass plugin. However, for some technical reasons that I cannot recall precisely, I did not choose this approach. It may have been due to uncertainty regarding whether every module would be built with the instrumentation pass.\nAnother approach that I took is to implement the instrumentation pass as a sanitizer, which is well-supported by both Linux and clang with -fsanitize=\u0026lt;sanitizer-name\u0026gt; option. Sanitizers are implemented as LLVM passes and are automatically loaded by clang. Existing sanitizers can be found in llvm-project/llvm/lib/Transforms/Instrumentation.\nTo add a new sanitizer, several modifications were made to Clang/LLVM and this change can be found in my github repo.\nThe final script to build the kernel with the instrumentation:\n#!/bin/bash -ve PROJ_DIR=${PWD}/.. LLVM_BUILD=${PROJ_DIR}/build/bin BINUTIL=~/util/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin export PATH=${LLVM_BUILD}:${BINUTIL}:$PATH export LLVM_COMPILER=clang export KERNEL=kernel8 export KCFLAGS=\u0026#34;-march=armv8.5-a -fsanitize=kdfi_instrument \u0026#34; pushd ../linux-5.5-kdfi BINUTILS_TARGET_PREFIX=aarch64-none-linux-gnu make \\ ARCH=arm64 CROSS_COMPILE=aarch64-none-linux-gnu- \\ HOSTCC=clang CC=clang -j12 popd 4. Connecting Analysis and Instrumentation Now the kernel can be analyzed and instrumented, but there is one more issue to solve: how do we connect the analysis and the instrumentation? A single LLVM pass for both analysis and instrumentation would be ideal. However, the kernel, as most C/C++ projects do, builds each module separately and links them together, which makes it difficult to perform cross-module analysis during kernel build.\nTo address this, I came up with a two-pass approach: One pass is for cross-module analysis that extracts the list of instructions from a pre-compiled whole-kernel bitcode (vmlinux.bc). Another is for instrumentation that is applied to each module to perform instrumentation for the previous identified instructions.\nThis requires a way to share the list of instructions between the two LLVM passes. Here, I simply used a simple text file containing raw dump of instructions in LLVM IR format. Each instruction is accompanied by minimal metadata such as function name and the instruction\u0026rsquo;s order in the function to accurately specify the intended instruction. While this method works, I believe there are more elegant ways to share the instructions between two passes.\nAdditionally, to avoid generating an excessively large file by dumping every instruction that need to be instrumented, a simple filter in the analysis pass excludes dumping instructions if they can be identified within each module. Consequently, the instrumentation pass also performs a simple intra-module analysis to identify the instructions filtered out from the list.\n5. Arm64 Environment Setup I used two types of testing environments: QEMU and Raspberry Pi 4B (4GB). QEMU is used for debugging and security evaluation, while Raspberry Pi is used for performance evaluation.\nFor the QEMU environment, I used buildroot (v2022.02.03) to build a root file system. The entire command to run QEMU is as follows:\nLINUX=$1 sudo ~/util/qemu-5.1.0/aarch64-softmmu/qemu-system-aarch64 \\ -machine virt,mte=on \\ -cpu max \\ -nographic -smp 1 \\ -hda /home/juhee/project/ppac/buildroot-2022.02.3/output/images/rootfs.ext4 \\ -kernel $LINUX/arch/arm64/boot/Image \\ -append \u0026#34;console=ttyAMA0 root=/dev/vda oops=panic panic_on_warn=1 panic=-1 ftrace_dump_on_oops=orig_cpu debug earlyprintk=serial slub_debug=UZ nokaslr \u0026#34; \\ -m 2G \\ -net user,hostfwd=tcp::10023-:22 \\ -net nic -s -S The emulated kernel can then be debugged with aarch64-none-linux-gnu-gdb.\nTo build the kernel for Raspberry Pi, I used the official Raspberry Pi firmware for the boot files and modules. The following script builds and prepares the boot files and modules of the instrumented kernel:\n#!/bin/bash -ve PROJ_DIR=${PWD} LLVM_BUILD=${PROJ_DIR}/build/bin BINUTIL=~/util/gcc-arm-10.3-2021.07-x86_64-aarch64-none-linux-gnu/bin export PATH=${LLVM_BUILD}:${BINUTIL}:$PATH export LLVM_COMPILER=clang export KERNEL=kernel8 export KCFLAGS=\u0026#34;-fsanitize=kdfi_instrument \u0026#34; pushd linux-rpi-6.0-$1 BINUTILS_TARGET_PREFIX=aarch64-none-linux-gnu make \\ ARCH=arm64 CROSS_COMPILE=aarch64-none-linux-gnu- \\ CFLAGS=\u0026#34;-march=armv8-a+crc -mtune=cortex-a72\u0026#34; \\ CXXFLAGS=\u0026#34;-march=armv8-a+crc -mtune=cortex-a72\u0026#34; \\ HOSTCC=clang CC=clang -j10 \\ bindeb-pkg Image modules dtbs BINUTILS_TARGET_PREFIX=aarch64-none-linux-gnu make \\ ARCH=arm64 CROSS_COMPILE=aarch64-none-linux-gnu- \\ HOSTCC=clang CC=clang -j10 \\ CFLAGS=\u0026#34;-march=armv8-a+crc -mtune=cortex-a72\u0026#34; \\ CXXFLAGS=\u0026#34;-march=armv8-a+crc -mtune=cortex-a72\u0026#34; \\ INSTALL_MOD_PATH=../modules-$2 \\ modules_install cp arch/arm64/boot/Image ../boot-$2/kernel8.img cp arch/arm64/boot/dts/overlays/*.dtbo ../boot-$2/overlays cp arch/arm64/boot/dts/overlays/README ../boot-$2/overlays cp arch/arm64/boot/dts/broadcom/*.dtb ../boot-$2/ popd tar -cvf modules-$2.tar.gz modules-$2/ tar -cvf boot-$2.tar.gz boot-$2/ *.deb The boot files and modules can be sent to the Raspberry Pi with network, and then can be installed with the following script:\n#!/bin/bash -ve rm *.deb tar -xvf boot-$1.tar.gz sudo rm -r /boot/* sudo dpkg -i linux-headers-*.deb linux-image-*.deb linux-libc-*.deb sudo cp -r boot-$1/* /boot/ sudo cp *.txt /boot/ tar -xvf modules-$1.tar.gz sudo cp -r modules-$1/lib/* /lib/modules/ Conclusion This post described the issues I faced when building a security hardening system for the Linux kernel. I hope this post will be helpful for others facing similar issues.\n","href":"/posts/linux-arm64-hardening/","title":"Security-hardening ARM64 Linux kernel"},{"content":"","href":"/categories/","title":"Categories"},{"content":"","href":"/page/","title":"Pages"},{"content":"","href":"/authors/","title":"Authors"},{"content":"","href":"/tags/config/","title":"Configuration"},{"content":"","href":"/categories/graduate-life/","title":"Graduate Life"},{"content":"","href":"/tags/og/","title":"Opengraph"},{"content":"","href":"/categories/research/","title":"Research"},{"content":"","href":"/search/","title":"Search"},{"content":"","href":"/series/","title":"Series"}]
